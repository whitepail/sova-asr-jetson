[Wav2Letter]
# Prediction letters
labels = [_-абвгдеёжзийклмнопрстуфхцчшщъыьэюя ]

# Path to acoustic model
model_path = /sova-asr/data/w2l-8khz.hdf

# Path to language model
lm_path = /sova-asr/data/lm/lm.klm

# Path to the lexicon file
lexicon = /sova-asr/data/lm/lexicon.txt

# Path to prediction tokens file
tokens = /sova-asr/data/tokens.txt

# Use CPU for acoustic model inference
cpu = 0

# Use greedy decoder instead of CTC decoder
greedy = 0

# Beam threshold for CTC decoder
beam_threshold = 10

# Audio sample rate, should match the acoustic model sample rate
sample_rate = 8000

# Window size in seconds for acoustic model samples
window_size = 0.02

# Window stride in seconds for acoustic model samples
window_stride = 0.01

# Voice Activity Detector agressiveness mode (0-3)
vad_aggressiveness_mode = 3

# Voice Activity Detector frame duration in milliseconds
vad_frame_duration_ms = 10

# Voice Activity Detector maximum pause duration in milliseconds
vad_max_pause_ms = 500


[Train]
# Path to train manifest csv
train_manifest = data/train.csv

# Path to validation manifest csv
val_manifest = data/val.csv

# Number of training epochs
epochs = 100

# Batch size for training
batch_size = 8

# Initial learning rate
learning_rate = 1e-5

# Run training with half-precision for memory use optimization
fp16 = 1

# Name prefix for checkpoints
checkpoint_name = w2l

# Save checkpoints per batch, 0 means never save
checkpoint_per_batch = 1000

# Location to save checkpoints
save_folder = Checkpoints/

# Continue from checkpoint model
continue_from = data/w2l-16khz.hdf
